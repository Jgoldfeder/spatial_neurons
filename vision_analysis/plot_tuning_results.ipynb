{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Results Visualization\n",
    "\n",
    "This notebook visualizes results from distributed Bayesian optimization runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Use a compatible style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "    except:\n",
    "        pass  # Use default style\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 studies:\n",
      "  - l1_distributed_cifar100_20260121_124638\n",
      "  - l1_distributed_cifar100_20260121_124644\n",
      "  - l1_distributed_cifar100_20260121_124726\n",
      "  - l1_distributed_cifar100_20260121_124824\n"
     ]
    }
   ],
   "source": [
    "# Find all study directories\n",
    "TUNE_DIR = 'tune_distributed'\n",
    "\n",
    "studies = sorted(glob(os.path.join(TUNE_DIR, '*')))\n",
    "print(f\"Found {len(studies)} studies:\")\n",
    "for s in studies:\n",
    "    print(f\"  - {os.path.basename(s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: tune_distributed/l1_distributed_cifar100_20260121_124824\n"
     ]
    }
   ],
   "source": [
    "# Select study to analyze (use most recent by default)\n",
    "STUDY_DIR = studies[-1] if studies else None\n",
    "print(f\"Analyzing: {STUDY_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_study_results(study_dir):\n    \"\"\"Load all trial results from a study directory.\"\"\"\n    results = []\n    \n    # Try new structure: metrics/<mode>/*.json\n    metrics_dir = os.path.join(study_dir, 'metrics')\n    if os.path.exists(metrics_dir):\n        for mode in os.listdir(metrics_dir):\n            mode_dir = os.path.join(metrics_dir, mode)\n            if os.path.isdir(mode_dir):\n                for f in glob(os.path.join(mode_dir, '*.json')):\n                    with open(f, 'r') as fp:\n                        data = json.load(fp)\n                        data['mode'] = mode\n                        data['trial_file'] = os.path.basename(f)\n                        results.append(data)\n    \n    # Try loading from results.json (contains all trial data)\n    if not results:\n        results_json = os.path.join(study_dir, 'results.json')\n        if os.path.exists(results_json):\n            with open(results_json, 'r') as f:\n                data = json.load(f)\n                \n            # Extract mode from config\n            config_path = os.path.join(study_dir, 'config.json')\n            mode = 'unknown'\n            if os.path.exists(config_path):\n                with open(config_path, 'r') as f:\n                    config = json.load(f)\n                    mode = config.get('mode', 'unknown')\n            \n            # Parse trials - check both 'trials' and 'all_trials' keys\n            trials_list = data.get('trials') or data.get('all_trials') or []\n            for trial in trials_list:\n                trial_data = {\n                    'mode': mode,\n                    'trial_id': trial.get('number'),\n                    'accuracy_after_pruning': trial.get('value'),\n                }\n                # Add params\n                if 'params' in trial:\n                    trial_data.update(trial['params'])\n                # Add user_attrs\n                if 'user_attrs' in trial:\n                    trial_data.update(trial['user_attrs'])\n                results.append(trial_data)\n    \n    return pd.DataFrame(results)\n\ndf = load_study_results(STUDY_DIR)\nprint(f\"Loaded {len(df)} trials\")\nif len(df) > 0:\n    print(f\"Columns: {list(df.columns)}\")\n    display(df.head())"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters from params dict if present\n",
    "if 'params' in df.columns:\n",
    "    params_df = pd.json_normalize(df['params'])\n",
    "    df = pd.concat([df.drop('params', axis=1), params_df], axis=1)\n",
    "    print(\"Extracted params\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accuracy data to plot\n"
     ]
    }
   ],
   "source": [
    "if len(df) > 0 and 'accuracy_after_pruning' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for mode in df['mode'].unique():\n",
    "        mode_df = df[df['mode'] == mode].copy()\n",
    "        mode_df = mode_df.reset_index(drop=True)\n",
    "        \n",
    "        # Plot all trials\n",
    "        axes[0].scatter(range(len(mode_df)), mode_df['accuracy_after_pruning'], \n",
    "                        label=f'{mode}', alpha=0.7, s=50)\n",
    "        \n",
    "        # Plot best so far\n",
    "        best_so_far = mode_df['accuracy_after_pruning'].cummax()\n",
    "        axes[1].plot(range(len(mode_df)), best_so_far, label=f'{mode}', linewidth=2)\n",
    "\n",
    "    axes[0].set_xlabel('Trial')\n",
    "    axes[0].set_ylabel('Accuracy After Pruning (%)')\n",
    "    axes[0].set_title('All Trials')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_xlabel('Trial')\n",
    "    axes[1].set_ylabel('Best Accuracy So Far (%)')\n",
    "    axes[1].set_title('Optimization Progress')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No accuracy data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Mode: Gamma vs Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No L1 trials found (or no gamma column)\n"
     ]
    }
   ],
   "source": [
    "l1_df = df[df['mode'] == 'l1'].copy() if 'mode' in df.columns else pd.DataFrame()\n",
    "\n",
    "if len(l1_df) > 0 and 'gamma' in l1_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gamma vs accuracy (log scale)\n",
    "    has_before = 'accuracy_before_pruning' in l1_df.columns and l1_df['accuracy_before_pruning'].notna().any()\n",
    "    \n",
    "    if has_before:\n",
    "        sc = axes[0].scatter(l1_df['gamma'], l1_df['accuracy_after_pruning'], \n",
    "                        c=l1_df['accuracy_before_pruning'], cmap='viridis', s=80, alpha=0.7)\n",
    "        cbar = plt.colorbar(sc, ax=axes[0])\n",
    "        cbar.set_label('Acc Before Pruning')\n",
    "    else:\n",
    "        axes[0].scatter(l1_df['gamma'], l1_df['accuracy_after_pruning'], s=80, alpha=0.7)\n",
    "    \n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_xlabel('Gamma (log scale)')\n",
    "    axes[0].set_ylabel('Accuracy After Pruning (%)')\n",
    "    axes[0].set_title('L1: Gamma vs Accuracy')\n",
    "    \n",
    "    # Before vs after pruning\n",
    "    if has_before:\n",
    "        sc = axes[1].scatter(l1_df['accuracy_before_pruning'], l1_df['accuracy_after_pruning'], \n",
    "                        c=l1_df['gamma'], cmap='plasma', s=80, alpha=0.7)\n",
    "        axes[1].plot([0, 100], [0, 100], 'k--', alpha=0.3, label='y=x')\n",
    "        axes[1].set_xlabel('Accuracy Before Pruning (%)')\n",
    "        axes[1].set_ylabel('Accuracy After Pruning (%)')\n",
    "        axes[1].set_title('L1: Before vs After Pruning')\n",
    "        cbar = plt.colorbar(sc, ax=axes[1])\n",
    "        cbar.set_label('Gamma')\n",
    "    else:\n",
    "        # Histogram of accuracies\n",
    "        axes[1].hist(l1_df['accuracy_after_pruning'], bins=20, edgecolor='black')\n",
    "        axes[1].set_xlabel('Accuracy After Pruning (%)')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('L1: Accuracy Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best trial\n",
    "    best_idx = l1_df['accuracy_after_pruning'].idxmax()\n",
    "    best = l1_df.loc[best_idx]\n",
    "    print(f\"\\nBest L1 trial:\")\n",
    "    print(f\"  Gamma: {best['gamma']:.2f}\")\n",
    "    if has_before:\n",
    "        print(f\"  Accuracy before pruning: {best['accuracy_before_pruning']:.2f}%\")\n",
    "    print(f\"  Accuracy after pruning: {best['accuracy_after_pruning']:.2f}%\")\n",
    "    if 'actual_sparsity' in best:\n",
    "        print(f\"  Actual sparsity: {best['actual_sparsity']:.2f}%\")\n",
    "else:\n",
    "    print(\"No L1 trials found (or no gamma column)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Mode: Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Spatial trials found (or no gamma_spatial column)\n"
     ]
    }
   ],
   "source": [
    "spatial_df = df[df['mode'] == 'spatial'].copy() if 'mode' in df.columns else pd.DataFrame()\n",
    "\n",
    "if len(spatial_df) > 0 and 'gamma_spatial' in spatial_df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    has_D = 'D' in spatial_df.columns\n",
    "    has_before = 'accuracy_before_pruning' in spatial_df.columns and spatial_df['accuracy_before_pruning'].notna().any()\n",
    "    \n",
    "    # Gamma_spatial vs accuracy\n",
    "    if has_D:\n",
    "        sc = axes[0, 0].scatter(spatial_df['gamma_spatial'], spatial_df['accuracy_after_pruning'], \n",
    "                                c=spatial_df['D'], cmap='viridis', s=80, alpha=0.7)\n",
    "        plt.colorbar(sc, ax=axes[0, 0], label='D')\n",
    "    else:\n",
    "        axes[0, 0].scatter(spatial_df['gamma_spatial'], spatial_df['accuracy_after_pruning'], s=80, alpha=0.7)\n",
    "    axes[0, 0].set_xscale('log')\n",
    "    axes[0, 0].set_xlabel('Gamma Spatial (log scale)')\n",
    "    axes[0, 0].set_ylabel('Accuracy After Pruning (%)')\n",
    "    axes[0, 0].set_title('Spatial: Gamma_spatial vs Accuracy')\n",
    "    \n",
    "    # D vs accuracy\n",
    "    if has_D:\n",
    "        sc = axes[0, 1].scatter(spatial_df['D'], spatial_df['accuracy_after_pruning'], \n",
    "                                c=spatial_df['gamma_spatial'], cmap='plasma', s=80, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('D')\n",
    "        axes[0, 1].set_ylabel('Accuracy After Pruning (%)')\n",
    "        axes[0, 1].set_title('Spatial: D vs Accuracy')\n",
    "        plt.colorbar(sc, ax=axes[0, 1], label='Gamma Spatial')\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'No D parameter', ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "    \n",
    "    # Gamma_l1 vs accuracy (if used)\n",
    "    if 'gamma_l1' in spatial_df.columns and spatial_df['gamma_l1'].max() > 0:\n",
    "        sc = axes[1, 0].scatter(spatial_df['gamma_l1'], spatial_df['accuracy_after_pruning'], \n",
    "                                c=spatial_df['gamma_spatial'], cmap='viridis', s=80, alpha=0.7)\n",
    "        axes[1, 0].set_xscale('log')\n",
    "        axes[1, 0].set_xlabel('Gamma L1 (log scale)')\n",
    "        axes[1, 0].set_ylabel('Accuracy After Pruning (%)')\n",
    "        axes[1, 0].set_title('Spatial: Gamma_L1 vs Accuracy')\n",
    "        plt.colorbar(sc, ax=axes[1, 0], label='Gamma Spatial')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No L1 penalty used', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Spatial: Gamma_L1 vs Accuracy')\n",
    "    \n",
    "    # Before vs after pruning\n",
    "    if has_before:\n",
    "        if has_D:\n",
    "            sc = axes[1, 1].scatter(spatial_df['accuracy_before_pruning'], spatial_df['accuracy_after_pruning'], \n",
    "                                    c=spatial_df['D'], cmap='viridis', s=80, alpha=0.7)\n",
    "            plt.colorbar(sc, ax=axes[1, 1], label='D')\n",
    "        else:\n",
    "            axes[1, 1].scatter(spatial_df['accuracy_before_pruning'], spatial_df['accuracy_after_pruning'], s=80, alpha=0.7)\n",
    "        axes[1, 1].plot([0, 100], [0, 100], 'k--', alpha=0.3)\n",
    "        axes[1, 1].set_xlabel('Accuracy Before Pruning (%)')\n",
    "        axes[1, 1].set_ylabel('Accuracy After Pruning (%)')\n",
    "        axes[1, 1].set_title('Spatial: Before vs After Pruning')\n",
    "    else:\n",
    "        axes[1, 1].hist(spatial_df['accuracy_after_pruning'], bins=20, edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Accuracy After Pruning (%)')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "        axes[1, 1].set_title('Spatial: Accuracy Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best trial\n",
    "    best_idx = spatial_df['accuracy_after_pruning'].idxmax()\n",
    "    best = spatial_df.loc[best_idx]\n",
    "    print(f\"\\nBest Spatial trial:\")\n",
    "    print(f\"  Gamma Spatial: {best['gamma_spatial']:.2f}\")\n",
    "    if 'gamma_l1' in best:\n",
    "        print(f\"  Gamma L1: {best['gamma_l1']:.2f}\")\n",
    "    if 'D' in best:\n",
    "        print(f\"  D: {best['D']:.4f}\")\n",
    "    if has_before:\n",
    "        print(f\"  Accuracy before pruning: {best['accuracy_before_pruning']:.2f}%\")\n",
    "    print(f\"  Accuracy after pruning: {best['accuracy_after_pruning']:.2f}%\")\n",
    "    if 'actual_sparsity' in best:\n",
    "        print(f\"  Actual sparsity: {best['actual_sparsity']:.2f}%\")\n",
    "else:\n",
    "    print(\"No Spatial trials found (or no gamma_spatial column)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare L1 vs Spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only one mode found or no accuracy data - comparison requires both L1 and Spatial results\n"
     ]
    }
   ],
   "source": [
    "if 'mode' in df.columns and len(df['mode'].unique()) > 1 and 'accuracy_after_pruning' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    modes = df['mode'].unique()\n",
    "    data_for_box = [df[df['mode'] == m]['accuracy_after_pruning'].dropna() for m in modes]\n",
    "    axes[0].boxplot(data_for_box, labels=modes)\n",
    "    axes[0].set_xlabel('Mode')\n",
    "    axes[0].set_ylabel('Accuracy After Pruning (%)')\n",
    "    axes[0].set_title('Accuracy Distribution by Mode')\n",
    "    \n",
    "    # Summary stats\n",
    "    summary = df.groupby('mode')['accuracy_after_pruning'].agg(['mean', 'std', 'max', 'count'])\n",
    "    \n",
    "    # Bar chart of best\n",
    "    x = np.arange(len(modes))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[1].bar(x - width/2, summary.loc[modes, 'mean'], width, label='Mean', \n",
    "                        yerr=summary.loc[modes, 'std'], capsize=5)\n",
    "    bars2 = axes[1].bar(x + width/2, summary.loc[modes, 'max'], width, label='Best')\n",
    "    \n",
    "    axes[1].set_xlabel('Mode')\n",
    "    axes[1].set_ylabel('Accuracy After Pruning (%)')\n",
    "    axes[1].set_title('L1 vs Spatial Comparison')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(modes)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary.to_string())\n",
    "else:\n",
    "    print(\"Only one mode found or no accuracy data - comparison requires both L1 and Spatial results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Trials Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to display\n"
     ]
    }
   ],
   "source": [
    "if len(df) > 0:\n",
    "    # Display sorted by accuracy\n",
    "    display_cols = ['mode', 'accuracy_before_pruning', 'accuracy_after_pruning', 'actual_sparsity']\n",
    "    if 'gamma' in df.columns:\n",
    "        display_cols.append('gamma')\n",
    "    if 'gamma_spatial' in df.columns:\n",
    "        display_cols.extend(['gamma_spatial', 'gamma_l1', 'D'])\n",
    "\n",
    "    display_cols = [c for c in display_cols if c in df.columns]\n",
    "    \n",
    "    if 'accuracy_after_pruning' in df.columns:\n",
    "        display(df[display_cols].sort_values('accuracy_after_pruning', ascending=False).head(20))\n",
    "    else:\n",
    "        display(df[display_cols].head(20))\n",
    "else:\n",
    "    print(\"No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Multiple Studies for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No studies found\n"
     ]
    }
   ],
   "source": [
    "def load_all_studies(tune_dir='tune_distributed'):\n",
    "    \"\"\"Load results from all studies.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for study_path in glob(os.path.join(tune_dir, '*')):\n",
    "        if os.path.isdir(study_path):\n",
    "            study_name = os.path.basename(study_path)\n",
    "            study_df = load_study_results(study_path)\n",
    "            if len(study_df) > 0:\n",
    "                study_df['study'] = study_name\n",
    "                all_results.append(study_df)\n",
    "    \n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "all_df = load_all_studies()\n",
    "if len(all_df) > 0:\n",
    "    print(f\"Loaded {len(all_df)} total trials from {all_df['study'].nunique()} studies\")\n",
    "else:\n",
    "    print(\"No studies found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data to compare\n"
     ]
    }
   ],
   "source": [
    "# Best results per study\n",
    "if len(all_df) > 0 and 'accuracy_after_pruning' in all_df.columns:\n",
    "    # Filter to rows with valid accuracy\n",
    "    valid_df = all_df[all_df['accuracy_after_pruning'].notna()]\n",
    "    if len(valid_df) > 0:\n",
    "        best_per_study = valid_df.loc[valid_df.groupby('study')['accuracy_after_pruning'].idxmax()]\n",
    "        cols = ['study', 'mode', 'accuracy_after_pruning']\n",
    "        if 'accuracy_before_pruning' in best_per_study.columns:\n",
    "            cols.insert(2, 'accuracy_before_pruning')\n",
    "        if 'actual_sparsity' in best_per_study.columns:\n",
    "            cols.append('actual_sparsity')\n",
    "        cols = [c for c in cols if c in best_per_study.columns]\n",
    "        print(\"Best trial from each study:\")\n",
    "        display(best_per_study[cols])\n",
    "    else:\n",
    "        print(\"No valid accuracy data\")\n",
    "else:\n",
    "    print(\"No data to compare\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
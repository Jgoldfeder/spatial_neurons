{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Auto-load all metrics data\n",
    "def load_all_metrics(metrics_dir='./metrics'):\n",
    "    \"\"\"\n",
    "    Automatically load all .pkl files from metrics directory.\n",
    "    Returns nested dict: data[dataset][model][mode][gamma] = metrics_dict\n",
    "    \"\"\"\n",
    "    data = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "    \n",
    "    for dataset in os.listdir(metrics_dir):\n",
    "        dataset_path = os.path.join(metrics_dir, dataset)\n",
    "        if not os.path.isdir(dataset_path):\n",
    "            continue\n",
    "        \n",
    "        for mode in os.listdir(dataset_path):\n",
    "            mode_path = os.path.join(dataset_path, mode)\n",
    "            if not os.path.isdir(mode_path):\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(mode_path):\n",
    "                if not filename.endswith('.pkl'):\n",
    "                    continue\n",
    "                \n",
    "                # Parse filename: mode:model:gamma.pkl\n",
    "                parts = filename.replace('.pkl', '').split(':')\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                \n",
    "                _, model, gamma = parts\n",
    "                \n",
    "                filepath = os.path.join(mode_path, filename)\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    metrics = pickle.load(f)\n",
    "                \n",
    "                # Convert keys to strings for consistency\n",
    "                metrics = {str(k): v for k, v in metrics.items()}\n",
    "                data[dataset][model][mode][gamma] = metrics\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load everything\n",
    "ALL_DATA = load_all_metrics()\n",
    "\n",
    "# Show what's available\n",
    "print('Available datasets:', list(ALL_DATA.keys()))\n",
    "for ds in ALL_DATA:\n",
    "    print(f'  {ds} models:', list(ALL_DATA[ds].keys()))\n",
    "    for model in ALL_DATA[ds]:\n",
    "        print(f'    {model} modes:', list(ALL_DATA[ds][model].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURE YOUR ANALYSIS HERE\n",
    "# ============================================\n",
    "\n",
    "DATASET = 'cifar100'\n",
    "MODEL = 'vit_tiny_patch16_224'\n",
    "\n",
    "# Pick which methods to compare\n",
    "METHODS = [\n",
    "    'L1',\n",
    "    'spatial',\n",
    "    'spatial-swap',\n",
    "    'block-4',\n",
    "    'block-16',\n",
    "]\n",
    "\n",
    "# Filter to only methods that exist in the data\n",
    "available_methods = list(ALL_DATA[DATASET][MODEL].keys())\n",
    "METHODS = [m for m in METHODS if m in available_methods]\n",
    "print(f'Using methods: {METHODS}')\n",
    "print(f'Available but not selected: {set(available_methods) - set(METHODS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-threshold-charts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHART A: Sparsity vs Accuracy at Fixed Thresholds\n",
    "# One chart per threshold (0.01, 0.001, 0.0001)\n",
    "# ============================================\n",
    "\n",
    "thresholds = ['0.01', '0.001', '0.0001']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, threshold in zip(axes, thresholds):\n",
    "    for method in METHODS:\n",
    "        points = []\n",
    "        for gamma, metrics in ALL_DATA[DATASET][MODEL][method].items():\n",
    "            if threshold in metrics:\n",
    "                acc = metrics[threshold].get('final_acc')\n",
    "                sparsity = metrics[threshold].get('percent_below_t')\n",
    "                if acc is not None and sparsity is not None:\n",
    "                    points.append((sparsity, acc, gamma))\n",
    "        \n",
    "        if points:\n",
    "            points.sort(key=lambda x: x[0])  # Sort by sparsity\n",
    "            x = [p[0] for p in points]\n",
    "            y = [p[1] for p in points]\n",
    "            ax.plot(x, y, 'o-', label=method, markersize=6)\n",
    "    \n",
    "    ax.set_xlabel('Sparsity (%)')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'Threshold = {threshold}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{DATASET} / {MODEL} - Accuracy vs Sparsity at Fixed Thresholds', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-per-sparsity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHART B: Best Accuracy per Sparsity Level\n",
    "# For each fixed sparsity level, find the best gamma for each method\n",
    "# ============================================\n",
    "\n",
    "sparsity_levels = [100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 5, 3, 2, 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for method in METHODS:\n",
    "    best_points = []\n",
    "    \n",
    "    for p in sparsity_levels:\n",
    "        p_key = str(p)\n",
    "        best_acc = None\n",
    "        \n",
    "        # Find the best gamma for this sparsity level\n",
    "        for gamma, metrics in ALL_DATA[DATASET][MODEL][method].items():\n",
    "            if p_key in metrics:\n",
    "                acc = metrics[p_key].get('final_acc')\n",
    "                if acc is not None:\n",
    "                    if best_acc is None or acc > best_acc:\n",
    "                        best_acc = acc\n",
    "        \n",
    "        if best_acc is not None:\n",
    "            # x-axis: 100 - p = percent pruned\n",
    "            best_points.append((100 - p, best_acc))\n",
    "    \n",
    "    if best_points:\n",
    "        best_points.sort(key=lambda x: x[0])\n",
    "        x = [p[0] for p in best_points]\n",
    "        y = [p[1] for p in best_points]\n",
    "        ax.plot(x, y, 'o-', label=method, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Sparsity (% weights pruned)')\n",
    "ax.set_ylabel('Best Accuracy (%)')\n",
    "ax.set_title(f'{DATASET} / {MODEL} - Best Accuracy at Each Sparsity Level\\n(Best gamma selected per method per sparsity level)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modularity-charts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHART C: Modularity vs Accuracy\n",
    "# CHART D: Modularity vs Sparsity\n",
    "# Using threshold 0.001\n",
    "# ============================================\n",
    "\n",
    "threshold = '0.001'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Chart C: Modularity vs Accuracy\n",
    "ax = axes[0]\n",
    "for method in METHODS:\n",
    "    points = []\n",
    "    for gamma, metrics in ALL_DATA[DATASET][MODEL][method].items():\n",
    "        if threshold in metrics:\n",
    "            acc = metrics[threshold].get('final_acc')\n",
    "            modularity = metrics[threshold].get('modularity')\n",
    "            if acc is not None and modularity is not None:\n",
    "                points.append((modularity, acc))\n",
    "    \n",
    "    if points:\n",
    "        points.sort(key=lambda x: x[0])\n",
    "        x = [p[0] for p in points]\n",
    "        y = [p[1] for p in points]\n",
    "        ax.plot(x, y, 'o-', label=method, markersize=6)\n",
    "\n",
    "ax.set_xlabel('Modularity (Q)')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title(f'Modularity vs Accuracy (threshold={threshold})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Chart D: Modularity vs Sparsity\n",
    "ax = axes[1]\n",
    "for method in METHODS:\n",
    "    points = []\n",
    "    for gamma, metrics in ALL_DATA[DATASET][MODEL][method].items():\n",
    "        if threshold in metrics:\n",
    "            sparsity = metrics[threshold].get('percent_below_t')\n",
    "            modularity = metrics[threshold].get('modularity')\n",
    "            if sparsity is not None and modularity is not None:\n",
    "                points.append((modularity, sparsity))\n",
    "    \n",
    "    if points:\n",
    "        points.sort(key=lambda x: x[0])\n",
    "        x = [p[0] for p in points]\n",
    "        y = [p[1] for p in points]\n",
    "        ax.plot(x, y, 'o-', label=method, markersize=6)\n",
    "\n",
    "ax.set_xlabel('Modularity (Q)')\n",
    "ax.set_ylabel('Sparsity (%)')\n",
    "ax.set_title(f'Modularity vs Sparsity (threshold={threshold})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{DATASET} / {MODEL}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "block-sparsity-charts",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CHART E: Block Sparsity at Same Accuracy Level\n# For each method, find the best block sparsity achievable at each accuracy level\n# ============================================\n\naccuracy_levels = [80, 75, 70, 65, 60, 55, 50]\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor method in METHODS:\n    points = []\n    \n    # Collect all (accuracy, block_sparsity) pairs across all gammas and sparsity levels\n    all_pairs = []\n    for gamma, metrics in ALL_DATA[DATASET][MODEL][method].items():\n        for key, m in metrics.items():\n            if isinstance(m, dict):\n                acc = m.get('final_acc')\n                bs = m.get('block_sparsity_reordered')\n                if acc is not None and bs is not None:\n                    all_pairs.append((acc, bs * 100))\n    \n    # For each accuracy level, find the best block sparsity among runs with acc >= that level\n    for target_acc in accuracy_levels:\n        # Find all pairs with accuracy >= target\n        valid = [(acc, bs) for acc, bs in all_pairs if acc >= target_acc]\n        if valid:\n            # Pick the one with highest block sparsity\n            best = max(valid, key=lambda x: x[1])\n            points.append((target_acc, best[1]))\n    \n    if points:\n        x = [p[0] for p in points]\n        y = [p[1] for p in points]\n        ax.plot(x, y, 'o-', label=method, markersize=8)\n\nax.set_xlabel('Minimum Accuracy (%)')\nax.set_ylabel('Best Block Sparsity Achieved (%)')\nax.set_title(f'{DATASET} / {MODEL} - Block Sparsity at Same Accuracy Level\\n(For each accuracy threshold, best block sparsity among runs with acc >= threshold)')\nax.legend()\nax.grid(True, alpha=0.3)\nax.invert_xaxis()  # Higher accuracy on left\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}